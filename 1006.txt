Computational Reproducibility at Exascale 2019 (CRE2019)

Reproducibility is an important concern in all areas of computation. As such, computational reproducibility is receiving increasing interest from a variety of parties who are concerned with different aspects of computational reproducibility. Computational reproducibility encompasses several concerns including the sharing of code and data, as well as reproducible numerical results which may depend on operating system, tools, levels of parallelism, and numerical effects. In addition, the publication of reproducible computational results motivates a host of computational reproducibility concerns that arise from the fundamental notion of reproducibility of scientific results that has normally been restricted to experimental science.

The workshop addresses issues in reproducibility that arise when computing at exascale. It will include issues of numerical reproducibility as well as approaches and best practices to sharing and running code and the reproducible dissemination of computational results. The workshop is meant to address the scope of the problems of computational reproducibility in HPC in general, and those anticipated as we scale up to exascale machines in the next decade. The participants of this workshop will include government, academic, and industry stakeholders; the goals of this workshop are to understand the current state of the problems that arise, what work is being done to deal with this issues, and what the community thinks the possible approaches to these problem are.

Computational Reproducibility at Exascale 2019 (CRE2019)

Reproducibility is an important concern in all areas of computation. As such, computational reproducibility is receiving increasing interest from a variety of parties who are concerned with different aspects of computational reproducibility. Computational reproducibility encompasses several concerns including the sharing of code and data, as well as reproducible numerical results which may depend on operating system, tools, levels of parallelism, and numerical effects. In addition, the publication of reproducible computational results motivates a host of computational reproducibility concerns that arise from the fundamental notion of reproducibility of scientific results that has normally been restricted to experimental science.

The workshop addresses issues in reproducibility that arise when computing at exascale. It will include issues of numerical reproducibility as well as approaches and best practices to sharing and running code and the reproducible dissemination of computational results. The workshop is meant to address the scope of the problems of computational reproducibility in HPC in general, and those anticipated as we scale up to exascale machines in the next decade. The participants of this workshop will include government, academic, and industry stakeholders; the goals of this workshop are to understand the current state of the problems that arise, what work is being done to deal with this issues, and what the community thinks the possible approaches to these problem are.

Reproducibility, Computability, and the Scientific Method
Reproducibility and Provenance of James Webb Space Telescope Data Products
Numerical Reproducibility Based on Minimal-Precision Validation
Reproducibility and Variable Precision Computing

Reproducibility Initiative
After three years of increasing momentum, the SC Reproducibility Initiative makes Artifact Description (AD) Appendices mandatory for all papers submitted to the SC19 Technical Program, but optional for workshop papers and posters. Artifact Evaluation (AE) Appendices are still optional.

AD/AE Appendices will now be auto-generated from author responses to a standard form, embedded in the SC online submission system.

In 2019, the SC Reproducibility Initiative introduces three new tracks/committees, under the SC Reproducibility Chair:

AD/AE Appendices
Reproducibility Challenge
Journal Special Issue
 

New SC Reproducibility Tracks/Committees
AD/AE Appendices
This committee has the following charge:

Implement a program of actions to mentor authors in the preparation of the appendices. This may include a webinar, blog post, in-person consultation at the conference, expanding the Reproducibility Initiative FAQ, etc.;
Inspect the appendices of submitted papers, follow artifact URLs to check their contents, and support authors with appendix improvement, if needed.
Reproducibility Challenge
This committee has the following charge:

Select a paper accepted to the previous conference to be used as source of the Reproducibility Challenge in the Student Cluster Competition (SCC) of the next conference;
Work with the authors of the selected paper to build the challenge benchmark for the SCC teams;
Continue working until the time of the conference in crafting the rules and requirements for the challenge, and ensure compatibility with the various hardware used by the SCC teams.
Journal Special Issue
This committee has the following charge:

Review full papers submitted by the SCC teams for publication in the journal special issue, reporting on the results of the efforts to reproduce the challenge benchmark;
Work with student teams to assist them in revision of their papers, for a successful final publication of the reports in the special issue.
 

AD/AE Appendix Form
All authors submitting to the SC19 Technical Papers program must complete the AD/AE Appendix form to describe the computational artifacts their results rely on: software, data, and hardware. The form is standard, and embedded in the conference submission system.

View sample AD/AE Appendix Form within the Papers submission form sample under Paper Artifact Description / Article Evaluation (AD/AE) Appendix.

Note: If your paper used no computational artifacts, you will respond “No” to the first question and be done.

Submissions will be double-blind reviewed, and reviewers will have all the information in the AD/AE form except for the URLs pointing to the artifacts. The paper and the appendix, including URLs to artifacts, will be available to the AD/AE Appendices Committee. The committee may provide feedback to authors in a double-open arrangement (authors and committee members are known to each other). The AD/AE Appendices Committee will not share any information with the Program Committee other than to confirm whether artifacts indicated as available are in fact available. “Available” means that the provided URLs properly resolve, and that the author-created artifacts are persistently archived and have a global, unique identifier. Accepted papers with available artifacts will display an ACM badge.

Additional notes:

The form targets both Artifact Description (required) and Evaluation (optional).
Availability of software, data, and hardware artifacts must be explicitly indicated.
Papers should still be self-contained: they should describe the experimental methodology enough for the Program Committee to evaluate the contribution.
Sample scripts for machine-generated environment data are provided here.
Authors are encouraged to familiarize themselves early with the AD/AE Appendix requirements. Contact us with your questions.
 

History of the SC Reproducibility Initiative
2015
The SC steering committee approved the reproducibility initiative. Authors of SC15 papers were invited to submit an AD Appendix after the conference: one paper did so, became the source for the SC16 Student Cluster Competition Reproducibility Challenge and the first SC paper to display an ACM badge.

2016
Authors submitting to the SC16 conference could optionally submit an AD Appendix: nine authors submitted one, three were finalists, and one was selected to become the source for the SC17 Student Cluster Competition Reproducibility Challenge.

2017
SC made the AD Appendix a requirement to be considered for the Best Paper or Best Student Paper awards. SC17 also introduced the Computational Results Analysis (CRA) Appendix. 40% of submitted and 50% of accepted papers included an AD appendix; nine submitted papers (six accepted) included a CRA Appendix.

2018
SC extended the option of submitting AD Appendices to Workshops and Posters. The CRA Appendix was renamed Artifact Evaluation (AE) Appendix, and limited to four pages. AD Appendices were limited to 2 pages and remained optional (but required for consideration as Best Paper/Best Student Paper, and also Best Poster/Best Student Poster).

2019
AD Appendices will be mandatory for all submissions. AE Appendices are still optional, and both will be submitted via a standard form in the conference submission system. Three new Technical Program tracks, with their respective committees and chairs, are introduced in support of the SC Reproducibility Initiative.

 

Rationale and Results of Reproducibility at SC
The SC Conference Series is committed to introducing activities that highlight, enhance, and reward participant efforts to improve reproducibility. Each year, authors have engaged more with the Reproducibility Initiative, and the benefits have become more clear to our community.

The AD/AE Appendices improve the quality of SC papers, and increase efficiency of our collective endeavors:

The AD appendix provides information (details of software, hardware and data) that should be part of any publication with computational results.
The new AD/AE Appendices form standardizes the items that authors should include.
If an SC paper used no computational artifacts (software, hardware, or data), the author will click “No” in answer to the first question, and be done.
In the AE Appendix, authors describe how they verified and validated their computational results, supporting their trustworthiness and providing best-practices descriptions to the community that can be adopted by others who want to improve their own efforts.
AD/AE Appendices are reviewed in a double-open format by a dedicated committee, independent of the Technical Program committee (which maintains a double-blind process).
 

Engaging the Student Cluster Competition
The SC Reproducibility Challenge committee, now a part of the Technical Program, works with the SC Reproducibility Chair to select a paper from the previous conference to be used as source of the Reproducibility Challenge the next year. This committee works with the authors of the selected paper to develop the challenge benchmark and ensure that SCC teams have the best chance at succeeding.

The SCC teams write extended reports of their results and their efforts with the Reproducibility Challenge, which are submitted for a journal special issue (where they are peer-reviewed before acceptance).

The SC paper selected for the Reproducibility Challenge receives a special recognition at the SC Awards Ceremony.


AD/AE Appendices Author FAQ
If you still have questions, you can email: ad-ae-appendices@info.supercomputing.org

Artifact Description (AD) Appendices are required for all papers submitted to the Technical Program at SC19. Artifact Evaluation (AE) Appendices are optional, but strongly encouraged.

AD and AE Appendix Requirements

Review Process

Impact of AD and AE Appendices

ACM Artifacts Available and Artifacts Evaluated Badges

AD and AE Appendix Requirements
Are AD and AE appendices required in order to submit to SC19?

An AD appendix is required for all Technical Program submissions. An AE appendix is optional but strongly encouraged.

Do I need to make my software open source in order to complete the AD appendix?

No. You are not asked to make any changes to your computing environment in order to complete the appendix. The AD appendix is meant to describe the computing environment in which you produced your results. Any author-created software does need to be open source, however, to be eligible for the ACM Artifacts Available badge (see below).

How should I format my AD Appendix

You don’t need to worry about formatting the Appendices. You will be presented with an online form during the paper submission with questions that you will answer directly on the submission site. After answering the questions, the system will automatically generate a PDF of the Appendix for you.

What information do I need to provide in the AD/AE Appendix online form?

A printout of the questions included in the AD/AE Appendix online form is provided in the Author-Kit repository. Be sure to familiarize yourself with these before writing your paper, and ideally before or while you are producing your results.

Review Process
Who will review my appendices?

The AD & AE Appendices will be reviewed with your paper by the Technical Program committee, but the artifact URLs will be removed from the version they review, as a precaution in support of double-blind review. In addition, the AD/AE Appendices Committee will review the unredacted appendices, and will check that artifacts are indeed available in the URLs provided. They will also help authors improve their appendices, in a double-open arrangement.

How will review of appendices interact with the double-blind review process?

The AD appendix should describe the data, software and hardware artifacts involved in producing the results. Reviewers could discover the author’s identity if they embark on an online search, but they will be asked not to, in support of double-blind review. Author-provided artifact URLs will be redacted from the appendices provided to the reviewers.

Impact of AD and AE Appendices
What’s the impact of an Artifact Description appendix on scientific reproducibility?

Reproducibility depends on, as a first step, sharing the provenance of results with transparency, and the AD appendix is an instrument of documentation and transparency. A good AD appendix helps researchers document their results, and helps other researchers build from them.

The paper text explains why I believe my answers are right and shows all my work. Why do I need to provide an Artifact Evaluation appendix?

There are many good reasons for formalizing the artifact evaluation process. Standard practice varies across disciplines, and SC is an international, multi-disciplinary conference. Labeling the evaluation as such improves our ability to review the paper and improves reader confidence in the veracity of the results when approaching the work from a different background.

Artifacts
What are “author created” artifacts and why make the distinction?

Author created artifacts are the hardware, software, or data created by the paper’s authors. Only these artifacts need be made available to facilitate reproducibility. Proprietary, closed source artifacts (e.g. commercial software and CPUs) will necessarily be part of many research studies. These proprietary artifacts should be described to the best of the author’s ability but do not need to be provided.

What about proprietary author-created artifacts?

The ideal case for reproducibility is to have all author created artifacts publically available with a stable identifier. Papers involving proprietary, closed source author-created artifacts should indicate the availability of the artifacts and describe them as much as possible. Note that results dependent on closed source artifacts are not reproducible and are therefore ineligible for most of the ACM’s artifact review badges. See https://www.acm.org/publications/policies/artifact-review-badging.

Are the numbers used to draw our charts a data artifact?

Not necessarily. Data artifacts are the data (input or output) required to reproduce the results, not necessarily the results themselves. For example, if your paper presents a system that generates charts from datasets then providing an input dataset would facilitate reproducibility. However, if the paper merely uses charts to elucidate results then the input data to whatever tool you used to draw those charts isn’t required to reproduce the paper’s results. The tool which drew the chart isn’t part of the study, so the input data to that tool is not a data artifact of this work.

Help! My data is HUGE! How do I make it publically available with a stable identifier?

Use Zenodo (https://help.zenodo.org/). Contact them for information on how to upload extremely large datasets. You can easily upload datasets of 50GB or less, have multiple datasets, and there is no size limit on communities.

What’s the impact of an Artifact Evaluation appendix on scientific reproducibility?

An artifact-evaluation effort can increase the trustworthiness of computational results. It can be particularly effective in the case of results obtained using specialized computing platforms, not available to other researchers. Leadership computing platforms, novel testbeds, and experimental computing environments are of keen interest to the supercomputing community. Access to these systems is typically limited, however. Thus, most reviewers cannot independently check results, and the authors themselves may be unable to recompute their own results in the future, given the impact of irreversible changes in the environment (compilers, libraries, components, etc.). The various forms of Artifact Evaluation improve confidence that computational results from these special platforms are correct.

ACM Artifacts Available and Artifacts Evaluated Badges
By the ACM Badging definitions:

ACM Artifacts Available badge: This badge is applied to papers in which associated artifacts have been made permanently available for retrieval.

The AD/AE Appendix form (new for SC19) will automatically determine eligibility for an ACM Artifacts Available badge on the basis of the answers to questions about the availability of author-created software, hardware or data products. The conditions of eligibility are:

All author-created software artifacts are maintained in a public repository under an OSI- approved license.
All author-created hardware artifacts are available and comply with the Open Source Hardware Definition.
All author-created data artifacts are maintained in a public repository with a stable identifier, such as a DOI.
Artifacts Evaluated badge: This badge is applied to papers whose associated artifacts have successfully completed an independent audit.

Reproducibility Initiative
We believe that reproducible science is essential, and that SC is a leader in this effort. Moving in this direction, we encourage authors to submit reproducibility information in the form of an Artifact Description (AD) Appendix along with their other submission materials in the SC submissions website.